\section{Introduction}
\subsection{Motivation}
With the rise of simulation and big data analytics as new foundations of all modern science,
access to large scale \ac{HPC} systems has become paramount for good research. Furthermore,
as the area of machine and deep learning progresses, with models containing over one billion
parameters \cite{gpt4param},  local computation becomes everless feasable, resulting in the 
rise of \ac{HPC} usage amongst various domains.\\

This report is written as an addition to my student research at the \ac{GWDG}. The \ac{GWDG} is the
data center of the University of Göttingen and the data and IT competence center of the 
Max Planck Society with its own \ac{HPC} department, running four large scale \ac{HPC} systems:
The \ac{SCC}, Emmy, Grete, and CARO:
\begin{itemize}
\item \textbf{\ac{SCC}}\cite{SCC}: The \ac{SCC} is the oldest cluster maintained 
by the \ac{GWDG}. Its user group is comprised of all researchers of the Max Planck Society as 
well as all (student) researchers of the University of Göttingen. It is a very heterogenous system,
based on several different CPUs and GPUs generations, located over several locations within 
Göttingen. In total, it has 18,376 CPU Cores, 99 TB RAM and 5.2 PiB of Storage.
\item \textbf{Emmy}\cite{Emmy}: Together with the Lise\footnote{
\url{https://www.zib.de/research_services/supercomputing}} cluster provided by the ZIB, Emmy is 
part of  the fourth supercomputer generation of the HLRN\footnote{Norddeutscher Verbund für Hoch- 
und Höchstleistungsrechnen \url{https://www.hlrn.de/}}. It can be used by all HLRN and NHR users.
Ranking 133th at the TOP500\footnote{As of November 2023.}, Emmy is the biggest cluster hosted by
the \ac{GWDG}. It consists of 111,464 CPU Cores distributed over 1423 compute nodes resulting 
in a total peak compute power of 5.95 PetaFLOP/s.
\item \textbf{Grete}: Grete is a GPU \ac{HPC} cluster, formally part of the NHR system.
The system partition features XXXXXXX with four NVIDIA A100 GPUs each, connected through fast
Infiniband fabric. Grete is OUTDATED AAA
%   - Grete
%     - Top 500 Ranking: and Green500
\item \textbf{CARO}\cite{CARO}: Analagously to Emmy provided for the NHR, CARO is a compute cluster
hosted for the DLR\footnote{Deutsches Zentrum für Luft- und Raumfahrt \url{https://www.dlr.de/en}}.
With its 1364 compute nodes with 175,744 CPU Cores and 3.46 PetaFLOP/s it ranks 228th on TOP500.
\end{itemize}


As the sheer number of nodes makes individually inspecting each one impossible, a centralized
monitoring solution is required. Beyond getting an basic understanding on which node is alive, 
monitoring systems serve several important purposes. With an aggregated view, system admins can
understand the usage pattern of users. Furthermore, it can be used as a means of load balancing 
for detecting preventable bottlenecks such as suboptimal job queue usage. Additionally, monitoring
allows for better demand analysis and forecasting, allowing for more efficient, just in time 
hardware upgrades.

\subsection{Goals and Contributions}

As the time of this writing, the GWDG has two different, Grafana-based monitoring solutions for 
the \ac{SCC} and Emmy/Grete. The goal of this report is to evaluate the performance viability of
unifying both monitoring solutions, replacing \ac{SCC}'s InfluxDB\footnote{
\url{https://www.influxdata.com/products/influxdb-overview/}} and Telegraf\footnote{
\url{https://www.influxdata.com/time-series-platform/telegraf/}} with Prometheus and 
\texttt{node\_exporter}.
As part of this, the following contributions were made:
\begin{itemize}
\item Designing a methodology for benchmarking a pull-based monitoring system.
\item Designing a methodology for benchmarking a pull-based monitoring client daemon, both in 
terms of throughput and the performance degradation caused to typical, throughput-oriented 
\ac{HPC} load.
\item Benchmarking the performance and scalability of Prometheus for a \ac{HPC} use case.
\item Benchmarking the performance and performance penalty of \texttt{node\_exporter} for a 
\ac{HPC} use case.
\end{itemize}

\subsubsection{Structure}
Starting with Section 2, the general topics of monitoring, time series databases and
Prometheus get introduced. Related work about time series database performance will be analyzed.
After that, in Section 3, the benchmark methodology for both Prometheus itself as well as
\texttt{node\_exporter} will be explained. Then, in Section 4, the results of those benchmarks
will be shown. After a short discussion in Section 5, the work will be concluded in Section 6.

\section{Background and Related Work}
\subsection{Monitoring}
% - typical architecture with graph
% - On Premise vs Cloud based
% - push vs pull
\subsubsection{Current Architecture: GWDG SCC}
% - TIG with Influxv2 and Flux
% - Still using InfluxQL
\subsubsection{Current Architecture: HLRN}
% - Grafana / Prometheus / Node Exporter 
\subsection{Time Series Databases}
\subsection{Prometheus}
% TODO prometheus is not just a tsdb, what is it
\subsection{TSDB Performance Evaluation}
% - TODO blabla there is a lot of nosql benchmarking research but not a lot on tsdb
% - TODO tsbs influx and victoriametrics
%   - Commercial incentives

\section{Benchmark Methodology}
% TODO bla bla which parts to benchmark
% Note that all code is available on GH
%   - Dont forget the second node exporter repo
\subsection{Setup}
% - HPC cluster and my laptop
% - Using default pre-built node exporter of version X
%   - Except for the patched metric throughput, where we forked at commit hash X
% - Using pre-built prometheus version 2.45.1 in an ubuntu 22.04 singularity container based on
%   the docker image available in the repo
% - 
\subsection{\texttt{node\_exporter}}
% - Welche Metriken wir beim starten aktivieren
\subsubsection{Metric Gathering}
% - Wie die Metriken mit green threads ge-fork-joined werden
% - TODO How we patched it instead of mocking for the most realistic version
%   - Show code in Appendix
\subsubsection{End to End}
% - We use two tools
%   - wrk TODO
%   - go-wrk TODO
% - All benchmarks are measured using `vmstat 1`
% - We have three types of benchmarks
%   - wrk seq
%     - Most realistic load, as one usually has a single prometheus
%     - simplest benchmark: we just blast as fast as possible with 1 thread
%   - wrk par
%     - It keeps the HTTP connections open for maximizing throughput.
%     - While it maximizes throughput, it is not realistic as prom wouldnt do that!
%     - scale along threads and HTTP connections
%   - go-wrk par
%     - it uses the same go `net` lib as prom
%     - It doesnt keep connections open but is highly parallel with go's green threading model
%     - Scale along go-threads
% - For the parallel benchmarks, we start it with some kind of parallelism for some amount
%   then increase parallelism in a step and so on and so fourth:
%   - We also scale along the axis of number of processe consumed by node exporter through the
%     GOMAXPROCS
%     - Note that even with n=1 go threads can still highly improve performance by fixing I/O idle
% - Fully automatic pipeline
\subsubsection{Jitter}
% - Why we think this could be a problem
%   - As mentioned before it spawns up several green threads
%   - Thus more memory footprint, thus potentially wasting the whole CPU cache
\subsection{Prometheus}
% - All benchmarks are measured using `vmstat 1`

% - We mocked node exporter
%   - TODO fastapi 0.104 uvicorn 0.24 single route
%   - TODO only integers, configurable amount of metrics, all nodes have same metrics
%   - TODO how we did randomness with urandom times mersenne warmup
%   - TODO how we count the requests to see whether sth is skipped
%     - Locking global counter
%     - asynccontextmanager
%     - unique filename through pid

% - High level (automatized) workflow.
%   - Give number of mock clients as cli param by providing a port range
%   - Create a dynamic prom cfg for those ports and a 10sec interval to bind-mount in via
%     singularity
%   - spawn all python mock processes in the port range, 1 process per port
%   - Start `vmstat 1`
%   - Start the singularity container with prometheus and the tmp config as well as the 
%     data directory binded in
%     - Explain that the data directory is needed for write ops in footnote
%   - sleep for the amount of the benchmark
%   - terminate prom, then vmstat, then all mock exporter
%     - Terminate == SIGTERM

\section{Results}
\subsection{\texttt{node\_exporter}}
\subsection{Prometheus}

\section{Discussion}
% Node Exporter:
% - we used /dev/urandom, not perlin noise. This is less realistic but less overhead
%   - Even more, the aforementioned tsbs has some predefined data gen
%     - Also they split up the step to avoid the overhead; for us too much IO overhead
%       - Note that we have a convert script to JSON in the repo

\section{Conclusion}
